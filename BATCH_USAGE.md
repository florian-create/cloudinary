# ğŸ“¸ Batch Screenshot - Guide d'utilisation

Script local optimisÃ© pour gÃ©nÃ©rer des milliers de screenshots en parallÃ¨le.

## ğŸ¯ Pourquoi utiliser ce script au lieu de l'API Render ?

| Aspect | API Render | Script Local |
|--------|------------|--------------|
| **Vitesse** | ~30h pour 3700 URLs | **1.5-3h** en parallÃ¨le |
| **CoÃ»t** | $7/mo + usage CPU | **Gratuit** (ton CPU) |
| **ContrÃ´le** | âŒ LimitÃ© | âœ… Pause/reprise, retry |
| **Cache** | âœ… Cloudinary | âœ… Cloudinary + vÃ©rif locale |
| **Progression** | âŒ Invisible | âœ… Temps rÃ©el avec ETA |
| **StabilitÃ©** | âš ï¸ Timeouts rÃ©seau | âœ… Retry automatique |

## ğŸš€ Installation

### 1. Installer les dÃ©pendances locales

```bash
cd /Users/florian/Desktop/cloudinary-fix
pip install -r requirements.txt
pyppeteer-install
```

### 2. PrÃ©parer ton CSV

Ton CSV doit avoir une colonne avec les URLs. Exemple :

```csv
company_name,website,country
Acme Corp,https://acme.com,France
Widget Inc,widget.io,USA
Tech Solutions,www.techsolutions.fr,France
```

**Note** : Le script accepte les URLs avec ou sans `https://`, avec ou sans `www.`

## ğŸ“ Utilisation

### Option A : Depuis un CSV (RECOMMANDÃ‰ pour 3700 URLs)

Ã‰dite `batch_screenshot_local.py` ligne 245-246 :

```python
# DÃ©commenter ces lignes
urls = load_urls_from_csv('ton_fichier.csv', url_column='website')

# Commenter la liste de test
# urls = [...]
```

Puis lance :

```bash
python batch_screenshot_local.py
```

### Option B : Liste manuelle (pour tester)

Le script contient dÃ©jÃ  une liste de test. Lance directement :

```bash
python batch_screenshot_local.py
```

## âš™ï¸ Configuration

Dans `batch_screenshot_local.py`, ligne 31-40 :

```python
CONFIG = {
    'concurrent_workers': 10,  # ğŸ”§ AJUSTER ICI
    'width': 1200,
    'height': 800,
    'quality': 80,
    'wait_after_load': 1.5
}
```

### Recommandations `concurrent_workers`

| Machine | RAM | Workers recommandÃ©s | Temps (3700 URLs) |
|---------|-----|--------------------:|------------------:|
| MacBook Air M1 | 8GB | **10** | ~3-4h |
| MacBook Pro M1/M2 | 16GB | **15-20** | ~1.5-2h |
| PC Gaming | 16GB+ | **20-30** | ~1-1.5h |
| Serveur | 32GB+ | **50+** | ~30-45min |

**âš ï¸ Attention** : Plus de workers = plus de RAM utilisÃ©e (~200-300MB par worker)

## ğŸ“Š Output

### Pendant l'exÃ©cution

```
ğŸš€ Starting batch processing of 3700 URLs
âš™ï¸  Concurrent workers: 10
ğŸ“ Output file: screenshot_results.csv

ğŸ” Checking cache...
âš¡ Cache HIT: https://google.com
âš¡ Cache HIT: https://github.com
ğŸ“Š Cache results: 1245/3700 already exist
ğŸ¯ Will generate: 2455 new screenshots

ğŸ“¸ Capturing: https://example.com
âœ… Success: https://example.com â†’ https://res.cloudinary.com/...
ğŸ“Š Progress: 523/3700 (14.1%) | âœ… 1768 | âŒ 12 | â±ï¸ 8.3/s | ETA: 6.4min
```

### RÃ©sultat final

**Fichier CSV gÃ©nÃ©rÃ©** : `screenshot_results.csv`

```csv
url,screenshot_url,status,error,timestamp
https://acme.com,https://res.cloudinary.com/.../acme-com.jpg,success,,2025-11-04T15:30:45
https://widget.io,https://res.cloudinary.com/.../widget-io.jpg,cached,,2025-11-04T15:30:45
https://bad-url.com,,error,Navigation timeout,2025-11-04T15:31:12
```

**Colonnes** :
- `url` : URL source
- `screenshot_url` : Lien Cloudinary (ou vide si erreur)
- `status` : `success` | `cached` | `error`
- `error` : Message d'erreur (si Ã©chec)
- `timestamp` : Horodatage

## ğŸ”„ Reprendre aprÃ¨s interruption

Si le script plante ou que tu l'interromps (Ctrl+C) :

1. **Relancer simplement** : Le cache Cloudinary est dÃ©jÃ  vÃ©rifiÃ©, donc les screenshots existants seront skip instantanÃ©ment
2. **Filtrer les erreurs** : Ouvre `screenshot_results.csv`, filtre par `status=error`, crÃ©e un nouveau CSV avec ces URLs, et relance

## ğŸ’¡ Astuces

### 1. Lancer la nuit
```bash
# Lancer en arriÃ¨re-plan
nohup python batch_screenshot_local.py > batch.log 2>&1 &

# Suivre la progression
tail -f batch.log
```

### 2. Traiter par lots
Pour 3700 URLs, tu peux dÃ©couper en lots de 500 :

```python
urls_batch_1 = urls[0:500]
urls_batch_2 = urls[500:1000]
# etc.
```

### 3. Ignorer les erreurs rÃ©currentes

Si certains domaines timeoutent toujours (ex: sites bloquant les bots), crÃ©e une blacklist :

```python
blacklist = ['problematic-site.com', 'another-bad-one.io']
urls = [u for u in urls if not any(b in u for b in blacklist)]
```

## ğŸ› DÃ©pannage

### Erreur : `Browser closed unexpectedly`
- RÃ©duis `concurrent_workers` (10 â†’ 5)
- Ferme les autres apps pour libÃ©rer de la RAM

### Trop lent
- Augmente `concurrent_workers` (10 â†’ 20)
- RÃ©duis `wait_after_load` (1.5 â†’ 1.0)

### Beaucoup d'erreurs "Navigation timeout"
- Augmente `navigation_timeout` (20000 â†’ 30000)

## ğŸ“ˆ Estimation de temps

**Formule** : `Temps = (URLs totales - Cached) / (Workers Ã— 2.5/s)`

Exemples pour 3700 URLs (0% cached) :

- 5 workers : ~5h
- 10 workers : ~2.5h
- 20 workers : ~1.2h
- 50 workers : ~30min

**Avec cache 30%** (dÃ©jÃ  fait) : Divise par ~1.5

## ğŸ†š Quand utiliser l'API vs Script local ?

| Utilise l'API Render | Utilise le script local |
|----------------------|------------------------|
| < 50 URLs | > 100 URLs |
| IntÃ©gration avec Clay | Batch processing |
| Pas de setup local | Tu as Python installÃ© |
| Screenshots Ã  la demande | Screenshots massifs planifiÃ©s |

## âœ… Checklist avant de lancer

- [ ] CSV prÃªt avec colonne URLs
- [ ] `pyppeteer-install` exÃ©cutÃ©
- [ ] `concurrent_workers` ajustÃ© selon ta RAM
- [ ] Assez d'espace disque (~50MB temporaire pour 3700 URLs)
- [ ] Pas d'apps lourdes en arriÃ¨re-plan
- [ ] Cloudinary config vÃ©rifiÃ©e (credentials dans le script)

PrÃªt Ã  lancer ! ğŸš€
